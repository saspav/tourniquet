{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2023-05-01T11:55:31.661184Z",
     "iopub.execute_input": "2023-05-01T11:55:31.662195Z",
     "iopub.status.idle": "2023-05-01T11:55:31.672692Z",
     "shell.execute_reply.started": "2023-05-01T11:55:31.662155Z",
     "shell.execute_reply": "2023-05-01T11:55:31.671502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tourniquet/all_df.csv\n/kaggle/input/tourniquet/sample_submission.csv\n/kaggle/input/tourniquet/all_df_fp2.csv\n/kaggle/input/tourniquet/all_df_fp.csv\n/kaggle/input/tourniquet/all_df_MV2.csv\n/kaggle/input/tourniquet/all_df_min_0.csv\n/kaggle/input/tourniquet/train.csv\n/kaggle/input/tourniquet/test.csv\n/kaggle/input/tourniquet/scores.logs\n/kaggle/working/scores.logs\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) \n",
    "# will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "filenames = glob(f'/kaggle/working/*.*')\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    if 'scores.logs' in filename:\n",
    "        continue\n",
    "    Path(filename).unlink()\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved \n",
    "# as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, \n",
    "# but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T11:55:31.676040Z",
     "iopub.execute_input": "2023-05-01T11:55:31.676425Z",
     "iopub.status.idle": "2023-05-01T11:55:31.683124Z",
     "shell.execute_reply.started": "2023-05-01T11:55:31.676388Z",
     "shell.execute_reply": "2023-05-01T11:55:31.682064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MY_DATA_PATH = Path('/kaggle/input/tourniquet')\n",
    "WORK_PATH = Path('.')\n",
    "PREDICTIONS_DIR = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T11:55:31.684886Z",
     "iopub.execute_input": "2023-05-01T11:55:31.685446Z",
     "iopub.status.idle": "2023-05-01T11:55:31.707713Z",
     "shell.execute_reply.started": "2023-05-01T11:55:31.685408Z",
     "shell.execute_reply": "2023-05-01T11:55:31.706614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "from time import time\n",
    "\n",
    "__import__(\"warnings\").filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def convert_seconds(time_apply):\n",
    "    # print(type(time_apply), time_apply)\n",
    "    try:\n",
    "        time_apply = float(time_apply)\n",
    "    except ValueError:\n",
    "        time_apply = 0\n",
    "    if isinstance(time_apply, (int, float)):\n",
    "        hrs = time_apply // 3600\n",
    "        mns = time_apply % 3600\n",
    "        sec = mns % 60\n",
    "        time_string = ''\n",
    "        if hrs:\n",
    "            time_string = f'{hrs:.0f} час '\n",
    "        if mns // 60 or hrs:\n",
    "            time_string += f'{mns // 60:.0f} мин '\n",
    "        return f'{time_string}{sec:.1f} сек'\n",
    "\n",
    "\n",
    "def print_time(time_start):\n",
    "    \"\"\"\n",
    "    Печать времени выполнения процесса\n",
    "    :param time_start: время запуска в формате time.time()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    time_apply = time() - time_start\n",
    "    print(f'Время обработки: {convert_seconds(time_apply)}')\n",
    "\n",
    "\n",
    "def print_msg(msg):\n",
    "    print(msg)\n",
    "    return time()\n",
    "\n",
    "\n",
    "def memory_compression(df, use_category=True, use_float=True):\n",
    "    \"\"\"\n",
    "    Изменение типов данных для экономии памяти\n",
    "    :param df: исходный ДФ\n",
    "    :param use_category: преобразовывать строки в категорию\n",
    "    :param use_float: преобразовывать float в пониженную размерность\n",
    "    :return: сжатый ДФ\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        # print(f'{col} тип: {df[col].dtype}', str(df[col].dtype)[:4])\n",
    "\n",
    "        if str(df[col].dtype)[:4] in 'datetime':\n",
    "            continue\n",
    "\n",
    "        elif str(df[col].dtype) not in ('object', 'category'):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if str(df[col].dtype)[:3] == 'int':\n",
    "                if col_min > np.iinfo(np.int8).min and \\\n",
    "                        col_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif col_min > np.iinfo(np.int16).min and \\\n",
    "                        col_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif col_min > np.iinfo(np.int32).min and \\\n",
    "                        col_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif col_min > np.iinfo(np.int64).min and \\\n",
    "                        col_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            elif use_float and str(df[col].dtype)[:5] == 'float':\n",
    "                if col_min > np.finfo(np.float16).min and \\\n",
    "                        col_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif col_min > np.finfo(np.float32).min and \\\n",
    "                        col_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "        elif use_category and str(df[col].dtype) == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print(f'Исходный размер датасета в памяти '\n",
    "          f'равен {round(start_mem, 2)} мб.')\n",
    "    print(f'Конечный размер датасета в памяти '\n",
    "          f'равен {round(end_mem, 2)} мб.')\n",
    "    print(f'Экономия памяти = {(1 - end_mem / start_mem):.1%}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T11:55:31.856773Z",
     "iopub.execute_input": "2023-05-01T11:55:31.857058Z",
     "iopub.status.idle": "2023-05-01T11:55:31.971913Z",
     "shell.execute_reply.started": "2023-05-01T11:55:31.857031Z",
     "shell.execute_reply": "2023-05-01T11:55:31.970923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    def __init__(self, use_catboost=False, numeric_columns=None, category_columns=None,\n",
    "                 drop_first=False, scaler=None, args_scaler=None):\n",
    "        \"\"\"\n",
    "        Преобразование данных\n",
    "        :param use_catboost: данные готовятся для catboost\n",
    "        :param numeric_columns: цифровые колонки\n",
    "        :param category_columns: категориальные колонки\n",
    "        :param drop_first: из dummy переменных удалить первую колонку\n",
    "        :param scaler: какой скайлер будем использовать\n",
    "        :param degree: аргументы для скайлера,\n",
    "        например: степень для полиномиального преобразования\n",
    "        \"\"\"\n",
    "        self.use_catboost = use_catboost\n",
    "        self.category_columns = [] if category_columns is None else category_columns\n",
    "        self.numeric_columns = [] if numeric_columns is None else numeric_columns\n",
    "        self.drop_first = drop_first\n",
    "        self.exclude_columns = []\n",
    "        self.new_columns = []\n",
    "        self.comment = {'drop_first': drop_first}\n",
    "        self.train_months = (7, 8, 9, 10,)\n",
    "        self.valid_months = (11, 12)\n",
    "        self.train_idxs = None\n",
    "        self.valid_idxs = None\n",
    "        self.transform_columns = None\n",
    "        self.scaler = scaler\n",
    "        self.args_scaler = args_scaler\n",
    "        self.preprocess_path_file = None\n",
    "        self.beep_outlet = None\n",
    "        self.gates_mask = [(-1, -1, -1), (-1, -1, 10), (-1, -1, 11), (3, 3, 4), (3, 3, 10),\n",
    "                           (3, 3, 10, 11), (4, 4, 3), (4, 4, 4), (4, 4, 5), (4, 4, 7),\n",
    "                           (4, 4, 8), (4, 4, 9, 9), (4, 4, 9, 9, 5, 5), (4, 7, 3), (4, 9, 9),\n",
    "                           (5, 5, 10), (5, 10, 11), (6, 3, 3), (6, 6, 5), (6, 6, 7),\n",
    "                           (6, 6, 9, 9), (6, 7, 3), (6, 9, 9), (7, 3, 3), (7, 3, 3, 10),\n",
    "                           (7, 3, 3, 10, 11), (7, 3, 3, 11), (7, 3, 10), (7, 5, 5),\n",
    "                           (7, 5, 5, 10), (7, 5, 5, 10, 11), (8, 8, 5), (7, 8, 8), (7, 9, 9),\n",
    "                           (7, 9, 9, 3, 3), (7, 9, 9, 5, 5), (7, 9, 9, 5, 5, 5),\n",
    "                           (7, 9, 9, 5, 5, 10), (9, 5, 5), (9, 5, 5, 10), (9, 9, 3),\n",
    "                           (9, 9, 5), (9, 9, 5, 5), (9, 9, 5, 5, 10), (9, 9, 15),\n",
    "                           (10, 11, 4, 4), (10, 11, 6, 6), (10, 11, 10), (10, 13, 13),\n",
    "                           (11, 4, 4), (11, 4, 4, 3), (11, 4, 4, 3, 3), (11, 4, 4, 3, 3, 10),\n",
    "                           (11, 4, 4, 4), (11, 4, 4, 4, 4), (11, 4, 4, 5), (11, 4, 4, 5, 5),\n",
    "                           (11, 4, 4, 5, 5, 10), (11, 4, 4, 7), (11, 4, 4, 7, 3, 3),\n",
    "                           (11, 4, 4, 7, 5), (11, 4, 4, 8, 8), (11, 4, 4, 9, 9),\n",
    "                           (11, 4, 4, 9, 9, 5), (11, 4, 4, 9, 9, 15), (11, 4, 4, 15),\n",
    "                           (11, 6, 6), (11, 6, 6, 5), (11, 6, 6, 6), (11, 6, 6, 9, 9),\n",
    "                           (11, 10, 11), (11, 10, 11, 4), (11, 11, 4, 4), (11, 11, 4, 4, 9),\n",
    "                           (11, 11, 10), (12, 12, 11), (12, 12, 11, 4), (13, 13, 4, 4),\n",
    "                           (13, 13, 6, 6), (13, 13, 10), (13, 13, 11), (13, 13, 12, 12),\n",
    "                           (13, 13, 12, 12, 11), (15, 3, 3), (15, 3, 3, 10),\n",
    "                           (15, 3, 3, 10, 11), (15, 9, 9), (15, 9, 9, 5, 5),\n",
    "                           ]\n",
    "        self.gates_M_V2 = [(-1, -1, -1), (-1, -1, -1, -1), (-1, -1, 10), (-1, -1, 11),\n",
    "                           (3, 3, 4), (3, 3, 10), (3, 3, 10, 11), (3, 3, 10, 11, 4),\n",
    "                           (3, 3, 10, 11, 6), (3, 4, 4), (3, 10, 11), (3, 10, 11, 4, 4),\n",
    "                           (3, 10, 11, 6), (4, 3, 3), (4, 3, 3, 10), (4, 4, 3), (4, 4, 4),\n",
    "                           (4, 4, 3, 3, 10), (4, 4, 4, 9), (4, 4, 5), (4, 4, 5, 5, 10),\n",
    "                           (4, 4, 7), (4, 4, 7, 3, 3), (4, 4, 7, 5), (4, 4, 8), (4, 4, 9, 9),\n",
    "                           (4, 4, 9, 9, 5, 5), (4, 4, 9, 9, 15), (4, 4, 15), (4, 5, 5),\n",
    "                           (4, 5, 5, 10), (4, 7, 3), (4, 7, 3, 3, 10), (4, 7, 5), (4, 8, 8),\n",
    "                           (4, 9, 9), (4, 9, 9, 5, 5), (4, 9, 9, 5, 5, 10), (4, 9, 9, 15),\n",
    "                           (5, 5, 5), (5, 5, 5, 10), (5, 5, 10), (5, 5, 10, 11), (5, 10, 11),\n",
    "                           (5, 5, 10, 11, 4, 4), (5, 5, 10, 11, 6), (5, 5, 10, 13),\n",
    "                           (5, 10, 11, 4, 4), (5, 10, 11, 4, 4, 9), (5, 10, 11, 6, 6),\n",
    "                           (5, 10, 13), (6, 3, 3), (6, 5, 5), (6, 6, 5), (6, 6, 6),\n",
    "                           (6, 6, 6, 6, 9), (6, 6, 7), (6, 6, 9, 9), (6, 7, 3), (6, 9, 9),\n",
    "                           (6, 9, 9, 5), (7, 3, 3), (7, 3, 3, 10), (7, 3, 3, 10, 11),\n",
    "                           (7, 3, 3, 10, 11, 4), (7, 3, 10), (7, 5, 5), (7, 5, 5, 10),\n",
    "                           (7, 5, 5, 10, 11), (7, 8, 8), (7, 9, 9), (7, 9, 9, 3, 3),\n",
    "                           (7, 9, 9, 5, 5), (7, 9, 9, 5, 5, 5), (7, 9, 9, 5, 5, 10),\n",
    "                           (8, 8, 3), (8, 8, 5), (9, 3, 3), (9, 5, 5), (9, 5, 5, 5, 5),\n",
    "                           (9, 5, 5, 10), (9, 5, 5, 10, 11), (9, 5, 5, 10, 11, 4), (9, 9, 3),\n",
    "                           (9, 5, 5, 10, 11, 6), (9, 5, 5, 10, 13), (9, 9, 5), (9, 9, 5, 5),\n",
    "                           (9, 9, 5, 5, 5, 5), (9, 9, 5, 5, 10), (9, 9, 15), (10, 11, 4, 4),\n",
    "                           (10, 11, 4, 4, 9, 9), (10, 11, 6, 6), (10, 11, 10), (10, 13, 13),\n",
    "                           (11, 4, 4), (11, 4, 4, 3), (11, 4, 4, 3, 3), (11, 4, 4, 3, 3, 10),\n",
    "                           (11, 4, 4, 4), (11, 4, 4, 4, 4), (11, 4, 4, 5), (11, 4, 4, 5, 5),\n",
    "                           (11, 4, 4, 5, 5, 10), (11, 4, 4, 7), (11, 4, 4, 7, 3, 3),\n",
    "                           (11, 4, 4, 7, 5), (11, 4, 4, 8, 8), (11, 4, 4, 9, 9), (11, 6, 6),\n",
    "                           (11, 4, 4, 9, 9, 5), (11, 4, 4, 9, 9, 15), (11, 4, 4, 15),\n",
    "                           (11, 6, 6, 5), (11, 6, 6, 6), (11, 6, 6, 9, 9), (11, 10, 11),\n",
    "                           (11, 10, 11, 4), (11, 11, 4, 4), (11, 11, 4, 4, 9), (12, 12, 11),\n",
    "                           (12, 11, 4, 4), (12, 12, 11, 4), (13, 4, 4), (13, 12, 12),\n",
    "                           (13, 13, 4, 4), (13, 13, 6, 6), (13, 13, 10), (13, 13, 11),\n",
    "                           (13, 13, 12, 12), (13, 13, 12, 12, 11), (15, 3, 3),\n",
    "                           (15, 3, 3, 10), (15, 3, 3, 10, 11), (15, 9, 9), (15, 9, 9, 5, 5),\n",
    "                           ]\n",
    "\n",
    "    def initial_preparation(self, df):\n",
    "        \"\"\"\n",
    "        Общая первоначальная подготовка данных\n",
    "        :param df: исходный ДФ\n",
    "        :return: обработанный ДФ\n",
    "        \"\"\"\n",
    "        df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "        df[\"time\"] = df[\"timestamp\"].dt.time\n",
    "        df[\"day\"] = df[\"timestamp\"].dt.day\n",
    "        df[\"week\"] = df[\"timestamp\"].dt.week\n",
    "        df[\"month\"] = df[\"timestamp\"].dt.month\n",
    "\n",
    "        df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "        df[\"min\"] = df[\"timestamp\"].dt.minute\n",
    "        df[\"sec\"] = df[\"timestamp\"].dt.second\n",
    "\n",
    "        df['minutes'] = df[\"hour\"] * 60 + df[\"min\"]\n",
    "        df['seconds'] = df.minutes * 60 + df[\"sec\"]\n",
    "\n",
    "        # 1-й день месяца\n",
    "        df[\"1day\"] = df[\"timestamp\"].dt.is_month_start.astype(int)\n",
    "        # 2-й день месяца\n",
    "        df[\"2day\"] = (df.day == 2).astype(int)\n",
    "        # предпоследний день месяца\n",
    "        df[\"last_day-1\"] = (df.day == df.timestamp.dt.daysinmonth - 1).astype(int)\n",
    "        # Последний день месяца\n",
    "        df[\"last_day\"] = df[\"timestamp\"].dt.is_month_end.astype(int)\n",
    "\n",
    "        df[\"weekday\"] = df[\"timestamp\"].dt.dayofweek  # День недели от 0 до 6\n",
    "        df[\"dayofweek\"] = df[\"weekday\"] + 1  # День недели от 1 до 7\n",
    "\n",
    "        # Метка выходного дня\n",
    "        df[\"is_weekend\"] = df[\"weekday\"].map(lambda x: 1 if x in (5, 6) else 0)\n",
    "\n",
    "        # метки \"график 2 через 2\"\n",
    "        df[\"DofY1\"] = (df[\"timestamp\"].dt.dayofyear % 4).apply(lambda x: int(x in (1, 2)))\n",
    "        df[\"DofY2\"] = (df[\"timestamp\"].dt.dayofyear % 4).apply(lambda x: int(x < 2))\n",
    "\n",
    "        # df['morning'] = df['hour'].map(lambda x: 1 if 6 <= x <= 10 else 0)\n",
    "        # df['daytime'] = df['hour'].map(lambda x: 1 if 11 <= x <= 17 else 0)\n",
    "        # df['evening'] = df['hour'].map(lambda x: 1 if 18 <= x <= 22 else 0)\n",
    "        # df['night'] = df['hour'].map(lambda x: 1 if 0 <= x <= 5 or x == 23 else 0)\n",
    "\n",
    "        # Подсчет количества срабатываний за день\n",
    "        df[\"beep_count\"] = df.groupby(\"date\").timestamp.transform(\"count\")\n",
    "        # Подсчет количества срабатываний за день по каждому gate_id\n",
    "        df[\"beep_gate\"] = df.groupby([\"date\", \"gate_id\"]).timestamp.transform(\"count\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cat_dummies(self, df):\n",
    "        \"\"\"\n",
    "        Отметка категориальных колонок --> str для catboost\n",
    "        OneHotEncoder для остальных\n",
    "        :param df: ДФ\n",
    "        :return: ДФ с фичами\n",
    "        \"\"\"\n",
    "        # если нет цифровых колонок --> заполним их\n",
    "        if self.category_columns and not self.numeric_columns:\n",
    "            self.numeric_columns = [col_name for col_name in df.columns\n",
    "                                    if col_name not in self.category_columns]\n",
    "        # если нет категориальных колонок --> заполним их\n",
    "        if self.numeric_columns and not self.category_columns:\n",
    "            self.category_columns = [col_name for col_name in df.columns\n",
    "                                     if col_name not in self.numeric_columns]\n",
    "\n",
    "        for col_name in self.category_columns:\n",
    "            if col_name in df.columns:\n",
    "                if self.use_catboost:\n",
    "                    df[col_name] = df[col_name].astype(str)\n",
    "                else:\n",
    "                    print(f'Трансформирую колонку: {col_name}')\n",
    "                    # Create dummy variables\n",
    "                    df = pd.get_dummies(df, columns=[col_name], drop_first=self.drop_first)\n",
    "\n",
    "                    self.new_columns.extend([col for col in df.columns\n",
    "                                             if col.startswith(col_name)])\n",
    "        return df\n",
    "\n",
    "    def apply_scaler(self, df):\n",
    "        \"\"\"\n",
    "        Масштабирование цифровых колонок\n",
    "        :param df: исходный ДФ\n",
    "        :return: нормализованный ДФ\n",
    "        \"\"\"\n",
    "        if not self.transform_columns:\n",
    "            self.transform_columns = self.numeric_columns\n",
    "        if self.scaler and self.transform_columns:\n",
    "            print(f'Применяю scaler: {self.scaler.__name__} '\n",
    "                  f'с аргументами: {self.args_scaler}')\n",
    "            args = self.args_scaler if self.args_scaler else tuple()\n",
    "            scaler = self.scaler(*args)\n",
    "            scaled_data = scaler.fit_transform(df[self.transform_columns])\n",
    "            if scaled_data.shape[1] != len(self.transform_columns):\n",
    "                print(f'scaler породил: {scaled_data.shape[1]} колонок')\n",
    "                new_columns = [f'pnf_{n:02}' for n in range(scaled_data.shape[1])]\n",
    "                df = pd.concat([df, pd.DataFrame(scaled_data, columns=new_columns)], axis=1)\n",
    "                self.exclude_columns.extend(self.transform_columns)\n",
    "            else:\n",
    "                df[self.transform_columns] = scaled_data\n",
    "\n",
    "            self.comment.update(scaler=self.scaler.__name__, args_scaler=self.args_scaler)\n",
    "        return df\n",
    "\n",
    "    def fit_gate_times(self, df, remake_gates_mask=False, use_gates_mask_V2=False):\n",
    "        \"\"\"\n",
    "        Получение паттернов прохода через турникеты\n",
    "        :param df: тренировочный ДФ\n",
    "        :param remake_gates_mask: получить шаблоны масок из трейна, иначе взять из класса\n",
    "        :param use_gates_mask_V2: использовать расширенный набор масок из класса\n",
    "        :return: ДФ с паттернами\n",
    "        \"\"\"\n",
    "        print('Ищу паттерны в данных...\\n')\n",
    "        current_user_id = prev_time = None\n",
    "        current_gate_times, current_gates = [], []\n",
    "        res_gate_times, result_gates = [], []\n",
    "\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            if current_user_id != row[\"user_id\"]:\n",
    "                if len(current_gate_times) >= 3:\n",
    "                    res_gate_times.append((current_user_id, current_gate_times))\n",
    "                    result_gates.append((current_user_id, current_gates))\n",
    "                current_gate_times, current_gates = [], []\n",
    "                current_user_id = row[\"user_id\"]\n",
    "                prev_time = row[\"timestamp\"]\n",
    "            delta = int((row[\"timestamp\"] - prev_time).total_seconds()) if prev_time else 0\n",
    "            prev_time = row[\"timestamp\"]\n",
    "            current_gate_times.append((row[\"gate_id\"], delta))\n",
    "            current_gates.append(row[\"gate_id\"])\n",
    "\n",
    "        if len(current_gate_times) >= 3:\n",
    "            res_gate_times.append((current_user_id, current_gate_times))\n",
    "            result_gates.append((current_user_id, current_gates))\n",
    "\n",
    "        gates_times = [tuple(zip(*gt)) for gt in\n",
    "                       [*map(lambda x: tuple(x[1]), res_gate_times)]]\n",
    "\n",
    "        if remake_gates_mask:\n",
    "            res_gate = []\n",
    "            for user_gates in result_gates:\n",
    "                gates = user_gates[1]\n",
    "                start_range = 3 if len(gates) < 5 else 4\n",
    "                for len_mask in range(start_range, 7):\n",
    "                    res_gate.extend([*zip(*[gates[i:] for i in range(len_mask)])])\n",
    "            res_cnt = Counter(res_gate)\n",
    "            prev_key = prev_cnt = None\n",
    "            find_gates_mask = []\n",
    "            for key, cnt in sorted(res_cnt.items()):\n",
    "                # количество шаблонов 1 или 2 - игнорируем,\n",
    "                # количество шаблонов 3 и 4 берем только длиной 3 и 4,\n",
    "                # количество шаблонов 5 берем длины 3, 4 и 5,\n",
    "                # количество шаблонов 6 и более берем все\n",
    "                if (cnt in (3, 4) and len(key) in (3, 4)) or (cnt == 5 and len(key) < 6) or (\n",
    "                        cnt > 5 and len(key) < cnt):\n",
    "                    # убираем дубли когда след шаблон отличается на последним турникетом\n",
    "                    # и количество шаблонов различается на 2 и менее\n",
    "                    if len(key) > 4 and prev_key == key[:-1] and abs(prev_cnt - cnt) < 3:\n",
    "                        prev_key, prev_cnt = key, cnt\n",
    "                        continue\n",
    "                    prev_key, prev_cnt = key, cnt\n",
    "                    find_gates_mask.append(key)\n",
    "            # заменим ручной отбор шаблонов на автоматический\n",
    "            self.gates_mask = find_gates_mask\n",
    "            # print(*self.gates_mask, sep='\\n')\n",
    "        if use_gates_mask_V2:\n",
    "            # заменим ручной отбор шаблонов на ручной отбор V2\n",
    "            self.gates_mask = self.gates_M_V2\n",
    "            # print(*self.gates_mask, sep='\\n')\n",
    "        print('Количество паттернов:', len(self.gates_mask))\n",
    "\n",
    "        df_gt = pd.DataFrame(columns=['mask'] + [f'dt_{i}' for i in range(6)])\n",
    "        for gates, times in tqdm(gates_times):\n",
    "            for mask in self.gates_mask:\n",
    "                for idx, sub_gates in enumerate(zip(*[gates[i:] for i in range(len(mask))])):\n",
    "                    if sub_gates == mask:\n",
    "                        row_df_gt = [mask] + [*times[idx:idx + len(mask)]] + [0] * 3\n",
    "                        df_gt.loc[len(df_gt)] = row_df_gt[:7]\n",
    "        df_gt['dt_0'] = 0\n",
    "        return df_gt\n",
    "\n",
    "    def group_gate_times(self, df_gt, replace_gates_mask=False):\n",
    "        \"\"\"\n",
    "        Получение временных интервалов для паттернов\n",
    "        :param df_gt: ДФ с паттернами\n",
    "        :param replace_gates_mask: заменить атрибут self.gates_mask\n",
    "        :return: список паттернов с временными интервалами\n",
    "        \"\"\"\n",
    "\n",
    "        # диапазон границ интервалов расширим вниз на 50% и вверх 20% - это сработало лучше,\n",
    "        # чем расширение границ вниз и вверх на 5%\n",
    "        def make_min_max(col):\n",
    "            min_col = min(col)\n",
    "            min_col = 0 if min_col < 10 else int(min_col * 0.5)  # добавил вот это\n",
    "            return min_col, int(max(col) * 1.2)\n",
    "\n",
    "        grp = df_gt.groupby('mask', as_index=False).agg(\n",
    "            min_max_0=('dt_0', lambda x: make_min_max(x)),\n",
    "            min_max_1=('dt_1', lambda x: make_min_max(x)),\n",
    "            min_max_2=('dt_2', lambda x: make_min_max(x)),\n",
    "            min_max_3=('dt_3', lambda x: make_min_max(x)),\n",
    "            min_max_4=('dt_4', lambda x: make_min_max(x)),\n",
    "            min_max_5=('dt_5', lambda x: make_min_max(x)),\n",
    "        )\n",
    "        result = []\n",
    "        grp_columns = grp.columns.to_list()\n",
    "        for _, row in grp.iterrows():\n",
    "            mask = row['mask']\n",
    "            result.append((mask, tuple(row[col] for col in grp_columns[1:len(mask) + 1])))\n",
    "\n",
    "        if replace_gates_mask:\n",
    "            self.gates_mask = result\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def find_gates(row, mask, times=None):\n",
    "        shift_gates = [f'g{i}' for i in range(len(mask) - 1, -len(mask), -1)]\n",
    "        gates = row[shift_gates].values\n",
    "        gates_times = None\n",
    "        if times:\n",
    "            shift_times = [f't{i}' for i in range(len(mask) - 1, -len(mask), -1)]\n",
    "            gates_times = row[shift_times].values\n",
    "        index_mask = -1\n",
    "        for idx, sub_gates in enumerate(zip(*[gates[i:] for i in range(len(mask))])):\n",
    "            # найден паттерн турникетов\n",
    "            if sub_gates == mask:\n",
    "                # если есть проверка по дельта времени прохода --> смотрим,\n",
    "                # чтобы дельта попадала в границы диапазона обученных паттернов\n",
    "                if times is None or (\n",
    "                        times and all(times[i][0] <= gates_times[idx + i] <= times[i][1]\n",
    "                                      for i in range(1, len(mask)))):\n",
    "                    # print('паттерн найден')\n",
    "                    index_mask = idx\n",
    "                    break\n",
    "                print('паттерн не найден, user_id:', row['user_id'], 'idx =', idx)\n",
    "                print('mask, gates:', mask, gates)\n",
    "                print('times, gates_times:', times, gates_times)\n",
    "        return index_mask >= 0\n",
    "\n",
    "    def fit(self, df, file_df=None, out_five_percent=False, remake_gates_mask=False):\n",
    "        \"\"\"\n",
    "        Формирование фич\n",
    "        :param df: исходный ФД\n",
    "        :param file_df: Предобработанный Файл .pkl с полным путём\n",
    "        :param out_five_percent: граница 5% при оптределении выбросов\n",
    "        :param remake_gates_mask: получить шаблоны масок из трейна, иначе взять из класса\n",
    "        :return: обработанный ДФ\n",
    "        \"\"\"\n",
    "        if file_df and file_df.suffix == '.pkl' and file_df.is_file():\n",
    "            df = pd.read_pickle(file_df)\n",
    "            return df\n",
    "\n",
    "        df = self.initial_preparation(df)\n",
    "\n",
    "        # данные для устранения выбросов, где рабочий день помечен как выходной и наоборот\n",
    "        tmp = df[['date', 'weekday', 'beep_count', 'is_weekend']].drop_duplicates()\n",
    "        tmp[\"weekend\"] = tmp[\"weekday\"].map(lambda x: 1 if x in (5, 6) else 0)\n",
    "        beep_cnt = tmp[tmp[\"weekend\"] == 1].beep_count\n",
    "        if out_five_percent:\n",
    "            self.beep_outlet = beep_cnt.quantile(0.975)  # 69.75\n",
    "        else:\n",
    "            self.beep_outlet = beep_cnt.quantile(0.75) + beep_cnt.std() * 1.5  # 98.7\n",
    "\n",
    "        # выделил shift по датам, чтобы случайно не зацепить переход между сутками\n",
    "        result = pd.DataFrame()\n",
    "        for flt_date in sorted(df[\"date\"].unique()):\n",
    "            tmp = df[df[\"date\"] == flt_date]\n",
    "            # формирование колонок с gate_id для 5 предыдущих и следующих строк\n",
    "            for i in range(5, -6, -1):\n",
    "                tmp[f'g{i}'] = tmp['gate_id'].shift(i, fill_value=-9)\n",
    "\n",
    "            # формирование колонок с timestamp для 5 предыдущих и следующих строк\n",
    "            for i in range(5, -6, -1):\n",
    "                tmp[f'ts{i}'] = tmp['timestamp'].shift(i)\n",
    "            tmp[f'ts6'] = tmp[f'ts5']\n",
    "            for i in range(5, -6, -1):\n",
    "                tmp[f't{i}'] = tmp[f'ts{i}'] - tmp[f'ts{i + 1}']\n",
    "                tmp[f't{i}'] = tmp[f't{i}'].map(lambda x: x.total_seconds())\n",
    "                tmp[f't{i}'].fillna(0, inplace=True)\n",
    "                tmp[f't{i}'] = tmp[f't{i}'].astype(int)\n",
    "\n",
    "            # удалить временные колонки с timestamp.shift()\n",
    "            tmp.drop([f'ts{i}' for i in range(6, -6, -1)], axis=1, inplace=True)\n",
    "\n",
    "            if not len(result):\n",
    "                result = tmp\n",
    "            else:\n",
    "                result = pd.concat([result, tmp])\n",
    "\n",
    "        df = result\n",
    "\n",
    "        # df.to_csv('df_ts.csv', sep=';')\n",
    "\n",
    "        if remake_gates_mask:\n",
    "            # получим паттерны\n",
    "            tmp_columns = ['user_id', 'timestamp', 'gate_id']\n",
    "            train_tmp = df[df.user_id > -1][tmp_columns]\n",
    "            self.fit_gate_times(train_tmp, remake_gates_mask=True)\n",
    "            print('Количество паттернов:', len(self.gates_mask))\n",
    "\n",
    "        start_time = print_msg('Поиск по шаблонам...')\n",
    "\n",
    "        tqdm.pandas()\n",
    "        for mask in self.gates_mask:\n",
    "            times = None\n",
    "            if len(mask) == 2:\n",
    "                mask, times = mask\n",
    "            mask_col = 'G' + '_'.join(map(str, mask))\n",
    "            print(f'Шаблон: {mask} колонка: {mask_col}')\n",
    "            df[mask_col] = df.progress_apply(\n",
    "                lambda row: self.find_gates(row, mask, times=times), axis=1).astype(int)\n",
    "\n",
    "        if self.preprocess_path_file:\n",
    "            df.to_pickle(self.preprocess_path_file)\n",
    "            df.to_csv(self.preprocess_path_file.with_suffix('.csv'))\n",
    "\n",
    "        print_time(start_time)\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, model_columns=None):\n",
    "        \"\"\"\n",
    "        Формирование фич\n",
    "        :param df: ДФ\n",
    "        :param model_columns: список колонок, которые будут использованы в модели\n",
    "        :return: ДФ с фичами\n",
    "        \"\"\"\n",
    "        df = self.initial_preparation(df)\n",
    "\n",
    "        # устранение выбросов, где рабочий день помечен как выходной и наоборот\n",
    "        if self.beep_outlet:\n",
    "            weekend_to_work = df[\"is_weekend\"].eq(1) & df[\"beep_count\"].gt(self.beep_outlet)\n",
    "            work_to_weekend = df[\"is_weekend\"].eq(0) & df[\"beep_count\"].lt(self.beep_outlet)\n",
    "            df.loc[weekend_to_work, 'is_weekend'] = 0\n",
    "            df.loc[work_to_weekend, 'is_weekend'] = 1\n",
    "\n",
    "        # выделение временных лагов между проходами через gate_id\n",
    "        lags = {'lag0': lambda x: not x, 'lag1': lambda x: x == 1, 'lag2': lambda x: x == 2,\n",
    "                'lag3': lambda x: x <= 3,\n",
    "                'lag4': lambda x: 2 < x <= 5,\n",
    "                'lag5': lambda x: 5 < x <= 15,\n",
    "                'lag6': lambda x: 15 < x <= 25,\n",
    "                'lag7': lambda x: 25 < x <= 36,\n",
    "                'lag8': lambda x: 36 < x <= 49,\n",
    "                'lag9': lambda x: 49 < x <= 79,\n",
    "                }\n",
    "\n",
    "        for col_name, lag_func in lags.items():\n",
    "            df[col_name] = (df['t0'].map(lag_func) | df['t-1'].map(lag_func)).astype(int)\n",
    "\n",
    "        # выделение временных лагов между одинаковыми gate_id\n",
    "        for col_name, lag_func in lags.items():\n",
    "            # (g1 g0 g-1) & (t0 t-1)\n",
    "            gate_prev = (df['g1'] == df['g0']) & df['t0'].map(lag_func)\n",
    "            gate_next = (df['g0'] == df['g-1']) & df['t-1'].map(lag_func)\n",
    "            df[col_name.replace('lag', 'dbl')] = (gate_prev | gate_next).astype(int)\n",
    "\n",
    "        # группировки для подсчета кол-ва --------------------------------\n",
    "        grp_month = df.groupby(['month'], as_index=False).agg(\n",
    "            counts=('timestamp', 'count'),\n",
    "            user_id_unique=('user_id', lambda x: x.nunique()),\n",
    "            date_unique=('date', lambda x: x.nunique())\n",
    "        )\n",
    "        grp_month['prs'] = grp_month['counts'] / grp_month['counts'].sum()\n",
    "\n",
    "        grp_week = df.groupby(['week'], as_index=False).agg(\n",
    "            counts=('timestamp', 'count'),\n",
    "            user_id_unique=('user_id', lambda x: x.nunique())\n",
    "        )\n",
    "        grp_week['prs'] = grp_week['counts'] / grp_week['counts'].sum()\n",
    "\n",
    "        grp_date = df.groupby(['date'], as_index=False).agg(\n",
    "            date_cnt=('timestamp', 'count')\n",
    "        )\n",
    "        grp_gate = df.groupby(['date', 'gate_id'], as_index=False).agg(\n",
    "            gate_cnt=('timestamp', 'count'),\n",
    "        )\n",
    "        # группировки -------------------------------------------------\n",
    "\n",
    "        # это реализовано в initial_preparation более красиво\n",
    "        # df = df.merge(grp_date, on=['date'], how='left')\n",
    "        # df = df.merge(grp_gate, on=['date', 'gate_id'], how='left')\n",
    "\n",
    "        if model_columns is None:\n",
    "            model_columns = df.columns.to_list()\n",
    "\n",
    "        if \"user_id\" not in model_columns:\n",
    "            model_columns.insert(0, \"user_id\")\n",
    "\n",
    "        self.train_idxs = df[df.month.isin(self.train_months)].index\n",
    "        self.valid_idxs = df[df.month.isin(self.valid_months)].index\n",
    "\n",
    "        df = self.cat_dummies(df)\n",
    "\n",
    "        df = self.apply_scaler(df)\n",
    "\n",
    "        model_columns.extend(self.new_columns)\n",
    "\n",
    "        exclude_columns = [col for col in self.exclude_columns if col in df.columns]\n",
    "        exclude_columns.extend(col for col in df.columns if col not in model_columns)\n",
    "\n",
    "        if exclude_columns:\n",
    "            df.drop(exclude_columns, axis=1, inplace=True)\n",
    "\n",
    "        self.exclude_columns = exclude_columns\n",
    "\n",
    "        # Переводим типы данных в минимально допустимые - экономим ресурсы\n",
    "        df = memory_compression(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df, file_df=None, out_five_percent=False,\n",
    "                      remake_gates_mask=False, model_columns=None):\n",
    "        \"\"\"\n",
    "        fit + transform\n",
    "        :param df: исходный ФД\n",
    "        :param file_df: Предобработанный Файл .pkl с полным путём\n",
    "        :param out_five_percent: граница 5% при оптределении выбросов\n",
    "        :param remake_gates_mask: получить шаблоны масок из трейна, иначе взять из класса\n",
    "        :param model_columns: список колонок, которые будут использованы в модели\n",
    "        :return: ДФ с фичами\n",
    "        \"\"\"\n",
    "        df = self.fit(df, file_df=file_df, out_five_percent=out_five_percent,\n",
    "                      remake_gates_mask=remake_gates_mask)\n",
    "        df = self.transform(df, model_columns=model_columns)\n",
    "        return df\n",
    "\n",
    "    def train_test_split(self, df, y=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Деление на обучающую и валидационную выборки\n",
    "        :param df: ДФ\n",
    "        :param y: целевая переменная\n",
    "        :param args: аргументы\n",
    "        :param kwargs: именованные аргументы\n",
    "        :return: x_train, x_valid, y_train, y_valid\n",
    "        \"\"\"\n",
    "        if any(key in kwargs for key in ('train_size', 'test_size')):\n",
    "            if 'test_size' in kwargs:\n",
    "                train_size = 1 - kwargs['test_size']\n",
    "            else:\n",
    "                train_size = kwargs['train_size']\n",
    "            if train_size > 0.99:\n",
    "                train_size = 0.99\n",
    "            train_rows = int(len(df) * train_size)\n",
    "            x_train = df.iloc[:train_rows]\n",
    "            x_valid = df.iloc[train_rows:]\n",
    "            if y is None:\n",
    "                y_train = y_valid = None\n",
    "            else:\n",
    "                y_train = y.iloc[:train_rows]\n",
    "                y_valid = y.iloc[train_rows:]\n",
    "        else:\n",
    "\n",
    "            x_train = df.loc[self.train_idxs]\n",
    "            x_valid = df.loc[self.valid_idxs]\n",
    "            if y is None:\n",
    "                y_train = y_valid = None\n",
    "            else:\n",
    "                y_train = y.loc[self.train_idxs]\n",
    "                y_valid = y.loc[self.valid_idxs]\n",
    "\n",
    "            self.comment.update(train_months=self.train_months,\n",
    "                                valid_months=self.valid_months)\n",
    "\n",
    "        return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    @staticmethod\n",
    "    def make_sample(df, days=7):\n",
    "        \"\"\"\n",
    "        Для опытов оставим небольшой сэмпл из данных и виде первых дней days\n",
    "        :param df: ДФ\n",
    "        :param days: количество дней для обучения и +1 день для теста, чтобы код не падал\n",
    "        :return: ДФ сеэмпла данных\n",
    "        \"\"\"\n",
    "        dates = sorted(df['date'].unique())[:days + 1]\n",
    "        temp = df[df['date'].isin(dates)]\n",
    "        temp.loc[temp['date'] == dates[-1], 'user_id'] = -1\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T11:55:31.976030Z",
     "iopub.execute_input": "2023-05-01T11:55:31.976306Z",
     "iopub.status.idle": "2023-05-01T11:55:32.009518Z",
     "shell.execute_reply.started": "2023-05-01T11:55:31.976280Z",
     "shell.execute_reply": "2023-05-01T11:55:32.003985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_train_valid(model, datasets, label_enc=None):\n",
    "    X_train, X_valid, y_train, y_valid, train, target, test_df = datasets\n",
    "    valid_pred = model.predict(X_valid)\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_full = model.predict(train)\n",
    "\n",
    "    if len(valid_pred.shape) > 1 and valid_pred.shape[1] > 1:\n",
    "        predict_proba = valid_pred.copy()\n",
    "        valid_pred = np.argmax(valid_pred, axis=1)\n",
    "    else:\n",
    "        predict_proba = model.predict_proba(X_valid)\n",
    "\n",
    "    if len(train_pred.shape) > 1 and train_pred.shape[1] > 1:\n",
    "        train_pred = np.argmax(train_pred, axis=1)\n",
    "    if len(train_full.shape) > 1 and train_full.shape[1] > 1:\n",
    "        train_full = np.argmax(train_full, axis=1)\n",
    "\n",
    "    f1w = f1_score(y_valid, valid_pred, average='weighted')\n",
    "    acc_valid = accuracy_score(y_valid, valid_pred)\n",
    "    acc_train = accuracy_score(y_train, train_pred)\n",
    "    acc_full = accuracy_score(target, train_full)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_valid, predict_proba,\n",
    "                                average='weighted', multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = 0\n",
    "\n",
    "    # print(classification_report(y_valid, valid_pred))\n",
    "    return acc_train, acc_valid, acc_full, roc_auc, f1w\n",
    "\n",
    "\n",
    "def predict_test(idx_fold, model, datasets, max_num=0, submit_prefix='lg_', \n",
    "                 label_enc=None, save_predict_proba=True):\n",
    "    X_train, X_valid, y_train, y_valid, train, target, test_df = datasets\n",
    "    # постфикс если было обучение на отдельных фолдах\n",
    "    nfld = f'_{idx_fold}' if idx_fold else ''\n",
    "    predictions = model.predict(test_df)\n",
    "    predict_train = model.predict(train)\n",
    "\n",
    "    if label_enc:\n",
    "        predictions = label_enc.inverse_transform(predictions)\n",
    "        predict_train = label_enc.inverse_transform(predict_train)\n",
    "\n",
    "    # печать размерности предсказаний и списка меток классов\n",
    "    classes = model.classes_.tolist()\n",
    "    print('predict_proba.shape:', predictions.shape, 'classes:', classes)\n",
    "\n",
    "    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
    "        predict_proba = predictions.copy()\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        train_proba = predict_train.copy()\n",
    "        predict_train = np.argmax(predict_train, axis=1)\n",
    "    else:\n",
    "        predict_proba = model.predict_proba(test_df)\n",
    "        train_proba = model.predict_proba(train)\n",
    "\n",
    "    submit_csv = f'{submit_prefix}submit_{max_num:03}{nfld}.csv'\n",
    "    file_submit_csv = PREDICTIONS_DIR.joinpath(submit_csv)\n",
    "    file_proba_csv = PREDICTIONS_DIR.joinpath(submit_csv.replace('submit_', 'proba_'))\n",
    "    file_train_csv = PREDICTIONS_DIR.joinpath(submit_csv.replace('submit_', 'train_'))\n",
    "\n",
    "    # Write the predictions to a file\n",
    "    submit = test_df[['1day', '2day']]\n",
    "    submit['target'] = predictions\n",
    "    submit[['target']].to_csv(file_submit_csv)\n",
    "\n",
    "    if save_predict_proba:\n",
    "        train_sp = pd.DataFrame(target)\n",
    "        train_sp['target'] = predict_train\n",
    "        train_sp.to_csv(file_train_csv)\n",
    "\n",
    "        try:\n",
    "            train_sp[classes] = train_proba\n",
    "            train_sp.to_csv(file_train_csv)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            proba = submit[['target']]\n",
    "            proba[classes] = predict_proba\n",
    "            proba.to_csv(file_proba_csv)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    acc_train, acc_valid, acc_full, roc_auc, f1w = predict_train_valid(model, datasets,\n",
    "                                                                       label_enc=label_enc)\n",
    "\n",
    "    print(f'Accuracy = {acc_valid:.6f}')\n",
    "    print(f'Weighted F1-score = {f1w:.6f}')\n",
    "\n",
    "    print(f'Accuracy train:{acc_train} valid:{acc_valid} full:{acc_full} roc_auc:{roc_auc}')\n",
    "    return acc_train, acc_valid, acc_full, roc_auc, f1w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T11:55:32.016060Z",
     "iopub.execute_input": "2023-05-01T11:55:32.016334Z",
     "iopub.status.idle": "2023-05-01T12:03:13.878946Z",
     "shell.execute_reply.started": "2023-05-01T11:55:32.016308Z",
     "shell.execute_reply": "2023-05-01T12:03:13.877723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение Catboost классификатор...\nИсходный размер датасета в памяти равен 41.55 мб.\nКонечный размер датасета в памяти равен 5.66 мб.\nЭкономия памяти = 86.4%\nОбучаюсь на колонках: ['user_id', 'gate_id', 'hour', 'min', 'seconds', '1day', '2day', 'last_day-1', 'last_day', 'weekday', 'is_weekend', 'DofY1', 'beep_count', 'beep_gate', 'G-1_-1_-1', 'G-1_-1_10', 'G-1_-1_11', 'G3_3_4', 'G3_3_10', 'G3_3_10_11', 'G4_4_3', 'G4_4_4', 'G4_4_5', 'G4_4_7', 'G4_4_8', 'G4_4_9_9', 'G4_4_9_9_5_5', 'G4_7_3', 'G4_9_9', 'G5_5_10', 'G5_10_11', 'G6_3_3', 'G6_6_5', 'G6_6_7', 'G6_6_9_9', 'G6_7_3', 'G6_9_9', 'G7_3_3', 'G7_3_3_10', 'G7_3_3_10_11', 'G7_3_3_11', 'G7_3_10', 'G7_5_5', 'G7_5_5_10', 'G7_5_5_10_11', 'G7_8_8', 'G7_9_9', 'G7_9_9_3_3', 'G7_9_9_5_5', 'G7_9_9_5_5_5', 'G7_9_9_5_5_10', 'G8_8_5', 'G9_5_5', 'G9_5_5_10', 'G9_9_3', 'G9_9_5', 'G9_9_5_5', 'G9_9_5_5_10', 'G9_9_15', 'G10_11_4_4', 'G10_11_6_6', 'G10_11_10', 'G10_13_13', 'G11_4_4', 'G11_4_4_3', 'G11_4_4_3_3', 'G11_4_4_3_3_10', 'G11_4_4_4', 'G11_4_4_4_4', 'G11_4_4_5', 'G11_4_4_5_5', 'G11_4_4_5_5_10', 'G11_4_4_7', 'G11_4_4_7_3_3', 'G11_4_4_7_5', 'G11_4_4_8_8', 'G11_4_4_9_9', 'G11_4_4_9_9_5', 'G11_4_4_9_9_15', 'G11_4_4_15', 'G11_6_6', 'G11_6_6_5', 'G11_6_6_6', 'G11_6_6_9_9', 'G11_10_11', 'G11_10_11_4', 'G11_11_4_4', 'G11_11_4_4_9', 'G11_11_10', 'G12_12_11', 'G12_12_11_4', 'G13_13_4_4', 'G13_13_6_6', 'G13_13_10', 'G13_13_11', 'G13_13_12_12', 'G13_13_12_12_11', 'G15_3_3', 'G15_3_3_10', 'G15_3_3_10_11', 'G15_9_9', 'G15_9_9_5_5', 'DofY2', 'lag0', 'lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag8', 'dbl0', 'dbl1', 'dbl2', 'dbl3', 'dbl4', 'dbl5', 'dbl6', 'dbl7', 'dbl8']\nКатегорийные колонки: ['gate_id', 'weekday', 'hour']\nИсключенные колонки: ['timestamp', 'date', 'time', 'day', 'week', 'month', 'dayofweek', 'sec', 'minutes', 'lag9', 'dbl9', 'm1', 'm2', 'm3', 'm4', 'd1', 'd2', 'd3', 'd4', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'g5', 'g4', 'g3', 'g2', 'g1', 'g0', 'g-1', 'g-2', 'g-3', 'g-4', 'g-5', 't5', 't4', 't3', 't2', 't1', 't0', 't-1', 't-2', 't-3', 't-4', 't-5']\nuser_id = 4\nuser_id = 51\nРазмер train_df = (37523, 121), test_df = (7125, 121)\ntest_size: 0.2 SEED=86\nLearning rate set to 0.148268\n0:\tlearn: 0.0697612\ttest: 0.0656720\tbest: 0.0656720 (0)\ttotal: 293ms\tremaining: 4m 52s\n50:\tlearn: 0.2595678\ttest: 0.2314301\tbest: 0.2314301 (50)\ttotal: 11.5s\tremaining: 3m 33s\n100:\tlearn: 0.3414681\ttest: 0.2842623\tbest: 0.2854610 (98)\ttotal: 23.2s\tremaining: 3m 26s\n150:\tlearn: 0.4039747\ttest: 0.3233376\tbest: 0.3233376 (150)\ttotal: 34.1s\tremaining: 3m 11s\n200:\tlearn: 0.4488305\ttest: 0.3459382\tbest: 0.3459382 (200)\ttotal: 45.2s\tremaining: 2m 59s\n250:\tlearn: 0.4879831\ttest: 0.3649161\tbest: 0.3649161 (250)\ttotal: 55.9s\tremaining: 2m 46s\n300:\tlearn: 0.5229013\ttest: 0.3877742\tbest: 0.3877742 (300)\ttotal: 1m 6s\tremaining: 2m 34s\n350:\tlearn: 0.5541640\ttest: 0.3951901\tbest: 0.3954688 (344)\ttotal: 1m 17s\tremaining: 2m 24s\n400:\tlearn: 0.5809358\ttest: 0.4129169\tbest: 0.4135492 (397)\ttotal: 1m 27s\tremaining: 2m 11s\n450:\tlearn: 0.6107512\ttest: 0.4260092\tbest: 0.4266492 (438)\ttotal: 1m 38s\tremaining: 1m 59s\n500:\tlearn: 0.6330746\ttest: 0.4376597\tbest: 0.4379424 (498)\ttotal: 1m 49s\tremaining: 1m 48s\n550:\tlearn: 0.6546126\ttest: 0.4483182\tbest: 0.4495338 (547)\ttotal: 1m 59s\tremaining: 1m 37s\n600:\tlearn: 0.6794393\ttest: 0.4647843\tbest: 0.4650002 (598)\ttotal: 2m 10s\tremaining: 1m 26s\n650:\tlearn: 0.7050432\ttest: 0.4775375\tbest: 0.4775375 (650)\ttotal: 2m 21s\tremaining: 1m 15s\n700:\tlearn: 0.7257811\ttest: 0.4836296\tbest: 0.4839684 (699)\ttotal: 2m 32s\tremaining: 1m 4s\n750:\tlearn: 0.7450857\ttest: 0.4939900\tbest: 0.4949667 (748)\ttotal: 2m 42s\tremaining: 53.8s\n800:\tlearn: 0.7618122\ttest: 0.5025927\tbest: 0.5027989 (797)\ttotal: 2m 53s\tremaining: 43.1s\n850:\tlearn: 0.7773436\ttest: 0.5095933\tbest: 0.5095933 (850)\ttotal: 3m 4s\tremaining: 32.3s\n900:\tlearn: 0.7908539\ttest: 0.5108497\tbest: 0.5129116 (895)\ttotal: 3m 16s\tremaining: 21.5s\n950:\tlearn: 0.8048081\ttest: 0.5193306\tbest: 0.5193306 (950)\ttotal: 3m 27s\tremaining: 10.7s\n999:\tlearn: 0.8185950\ttest: 0.5232555\tbest: 0.5239737 (975)\ttotal: 3m 37s\tremaining: 0us\nbestTest = 0.5239737177\nbestIteration = 975\nShrink model to first 976 iterations.\npredict_proba.shape: (7125, 1) classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]\nAccuracy = 0.525383\nWeighted F1-score = 0.523974\nAccuracy train:0.7922912918915318 valid:0.5253830779480346 full:0.7389068038269861 roc_auc:0.9456894895155148\nОбучаюсь на всём трейне...\n0:\tlearn: 0.0756287\ttotal: 266ms\tremaining: 4m 44s\n50:\tlearn: 0.2655187\ttotal: 11.6s\tremaining: 3m 51s\n100:\tlearn: 0.3511945\ttotal: 22.3s\tremaining: 3m 34s\n150:\tlearn: 0.4086881\ttotal: 32.7s\tremaining: 3m 19s\n200:\tlearn: 0.4501656\ttotal: 43.7s\tremaining: 3m 9s\n250:\tlearn: 0.4892001\ttotal: 53.6s\tremaining: 2m 55s\n300:\tlearn: 0.5243773\ttotal: 1m 3s\tremaining: 2m 42s\n350:\tlearn: 0.5551967\ttotal: 1m 14s\tremaining: 2m 33s\n400:\tlearn: 0.5815169\ttotal: 1m 24s\tremaining: 2m 22s\n450:\tlearn: 0.6092635\ttotal: 1m 35s\tremaining: 2m 11s\n500:\tlearn: 0.6326026\ttotal: 1m 46s\tremaining: 2m 1s\n550:\tlearn: 0.6538280\ttotal: 1m 56s\tremaining: 1m 50s\n600:\tlearn: 0.6724895\ttotal: 2m 6s\tremaining: 1m 39s\n650:\tlearn: 0.6900056\ttotal: 2m 17s\tremaining: 1m 29s\n700:\tlearn: 0.7078171\ttotal: 2m 27s\tremaining: 1m 18s\n750:\tlearn: 0.7257306\ttotal: 2m 38s\tremaining: 1m 7s\n800:\tlearn: 0.7438162\ttotal: 2m 49s\tremaining: 57.4s\n850:\tlearn: 0.7582713\ttotal: 2m 59s\tremaining: 46.9s\n900:\tlearn: 0.7732313\ttotal: 3m 9s\tremaining: 36.2s\n950:\tlearn: 0.7870688\ttotal: 3m 20s\tremaining: 25.7s\n1000:\tlearn: 0.8004159\ttotal: 3m 29s\tremaining: 15s\n1050:\tlearn: 0.8119794\ttotal: 3m 38s\tremaining: 4.58s\n1072:\tlearn: 0.8181581\ttotal: 3m 43s\tremaining: 0us\npredict_proba.shape: (7125, 1) classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]\nAccuracy = 0.812791\nWeighted F1-score = 0.812386\nAccuracy train:0.804217469518289 valid:0.8127914723517655 full:0.8059323614849558 roc_auc:0.9898946358603794\nbest_params: {'clf_iters': 976, 'clf_lr': 0.14826799929141998, 'loss_function': 'MultiClass', 'random_seed': 86, 'eval_metric': 'TotalF1', 'task_type': 'GPU', 'devices': '0:1', 'early_stopping_rounds': 80, 'cat_features': ['gate_id', 'weekday', 'hour']}\nWeighted F1-score = 0.812386\nПараметры модели: {'loss_function': 'MultiClass', 'random_seed': 86, 'eval_metric': 'TotalF1', 'task_type': 'GPU', 'devices': '0:1', 'early_stopping_rounds': 80, 'cat_features': ['gate_id', 'weekday', 'hour']}\nВремя обработки: 7 мин 41.8 сек\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    param = {\n",
    "        \"loss_function\": trial.suggest_categorical(\"loss_function\",\n",
    "                                                   [\"MultiClass\",\n",
    "                                                    \"MultiClassOneVsAll\"]),\n",
    "        \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\",\n",
    "                                                        [None, \"Balanced\"]),\n",
    "        # \"iterations\": trial.suggest_int(\"iterations\", 200, 2000, step=200),\n",
    "        # \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.25, step=0.05),\n",
    "    }\n",
    "\n",
    "    gbm = CatBoostClassifier(cat_features=cat_columns,\n",
    "                             eval_metric='TotalF1',\n",
    "                             early_stopping_rounds=80,\n",
    "                             random_seed=17,\n",
    "                             task_type=\"GPU\",\n",
    "                             devices='0:1',\n",
    "                             **param)\n",
    "\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"TotalF1\")\n",
    "\n",
    "    gbm.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=80,\n",
    "        callbacks=[pruning_callback],\n",
    "    )\n",
    "\n",
    "    # evoke pruning manually.\n",
    "    pruning_callback.check_pruned()\n",
    "\n",
    "    accuracy = accuracy_score(y_valid, gbm.predict(X_valid))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "file_logs_read = MY_DATA_PATH.joinpath('scores.logs')\n",
    "file_logs = WORK_PATH.joinpath('scores.logs')\n",
    "\n",
    "if not file_logs.is_file():\n",
    "    df = pd.read_csv(file_logs_read, sep=';')\n",
    "    df.to_csv(file_logs, index=False, sep=';')\n",
    "else:\n",
    "    df = pd.read_csv(file_logs, sep=';')\n",
    "\n",
    "df.num = df.index + 1\n",
    "max_num = df.num.max()\n",
    "\n",
    "start_time = print_msg('Обучение Catboost классификатор...')\n",
    "\n",
    "file_dir = MY_DATA_PATH\n",
    "file_train = file_dir.joinpath('train.csv')\n",
    "file_test = file_dir.joinpath('test.csv')\n",
    "\n",
    "train_df = pd.read_csv(file_train, parse_dates=['timestamp'], index_col='row_id')\n",
    "# train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(file_test, parse_dates=['timestamp'], index_col='row_id')\n",
    "test_df.insert(0, 'user_id', -1)\n",
    "\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "\n",
    "cat_columns = ['gate_id', 'weekday',\n",
    "               'hour',\n",
    "#                'is_weekend', '1day', '2day', 'last_day-1', 'last_day',\n",
    "#                'DofY1'\n",
    "               ]\n",
    "# ['gate_id', 'weekday', 'is_weekend', '1day', '2day', 'last_day-1', 'last_day']\n",
    "\n",
    "data_cls = DataTransform(use_catboost=True,\n",
    "                         # numeric_columns=numeric_columns,\n",
    "                         category_columns=cat_columns,\n",
    "                         )\n",
    "\n",
    "# prefix_preprocess = ''\n",
    "prefix_preprocess = '_min_0'\n",
    "# prefix_preprocess = '_fp'\n",
    "# prefix_preprocess = '_fp2'\n",
    "# prefix_preprocess = '_MV2'\n",
    "data_cls.preprocess_path_file = file_dir.joinpath(f'preprocess_df{prefix_preprocess}.pkl')\n",
    "\n",
    "data_cls.exclude_columns = [\n",
    "    'timestamp', 'date', 'time', 'day', 'week', 'month',\n",
    "    'dayofweek',\n",
    "#     'min',\n",
    "    'sec',\n",
    "    'minutes',\n",
    "#     'seconds',\n",
    "    '3day', 'last_days',\n",
    "#     'DofY1',\n",
    "#     'DofY2',\n",
    "#     'lag0', 'lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag8', 'lag9',\n",
    "#     'dbl0', 'dbl1', 'dbl2', 'dbl3', 'dbl4', 'dbl5', 'dbl6', 'dbl7', 'dbl8', 'dbl9',\n",
    "#     'lag8', 'dbl8',\n",
    "    'lag9', 'dbl9',\n",
    "    'm1', 'm2', 'm3', 'm4', 'd1', 'd2', 'd3', 'd4', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6',\n",
    "    'g5', 'g4', 'g3', 'g2', 'g1', 'g0', 'g-1', 'g-2', 'g-3', 'g-4', 'g-5',\n",
    "    't5', 't4', 't3', 't2', 't1', 't0', 't-1', 't-2', 't-3', 't-4', 't-5',\n",
    "    'G7_3_3_11', 'G7_3_10', 'G8_8_5', 'G11_6_6_5', 'G11_11_10', 'G13_13_6_6',  # 2 и 4\n",
    "    'G6_6_7', 'G7_8_8', 'G11_11_4_4_9', 'G13_13_12_12_11', 'G15_3_3_10_11',  # 5\n",
    "#     'G-1_-1_10', 'G-1_-1_11', 'G3_3_4', 'G3_3_10_11_6', 'G3_10_11_6', 'G13_13_11',  # 6\n",
    "]\n",
    "# data_cls.beep_outlet = 98.7\n",
    "# all_df = data_cls.fit_transform(all_df, file_df=data_cls.preprocess_path_file)\n",
    "\n",
    "all_df = pd.read_csv(file_dir.joinpath(f'all_df{prefix_preprocess}.csv'),\n",
    "                     parse_dates=['timestamp'],\n",
    "                     index_col='row_id')\n",
    "\n",
    "data_cls.beep_outlet = 98.7\n",
    "all_df = data_cls.transform(all_df)\n",
    "\n",
    "train_df = all_df[all_df.user_id > -1]\n",
    "test_df = all_df[all_df.user_id < 0]\n",
    "\n",
    "print('Обучаюсь на колонках:', train_df.columns.to_list())\n",
    "print('Категорийные колонки:', cat_columns)\n",
    "print('Исключенные колонки:', data_cls.exclude_columns)\n",
    "\n",
    "# добавление user_id для валидации кто попался только один раз\n",
    "for user_id in train_df.user_id.unique():\n",
    "    if len(train_df.loc[train_df.user_id == user_id]) < 4:\n",
    "        print(f'user_id = {user_id}')\n",
    "        train_df = train_df.append(train_df.loc[train_df.user_id == user_id])\n",
    "\n",
    "print(f'Размер train_df = {train_df.shape}, test_df = {test_df.shape}')\n",
    "\n",
    "train = train_df.drop(['user_id'], axis=1)\n",
    "target = train_df['user_id']\n",
    "\n",
    "test_df.drop(['user_id'], axis=1, inplace=True)\n",
    "\n",
    "save_predict_proba = True\n",
    "\n",
    "test_sizes = (0.2,)\n",
    "# test_sizes =  (0.15, 0.2, 0.25)\n",
    "# test_sizes = np.linspace(0.16, 0.24, 9)\n",
    "# for num_iters in range(500, 701, 50):\n",
    "# for SEED in range(100):\n",
    "for test_size in test_sizes:\n",
    "# for max_leaves in range(20, 70, 5):    \n",
    "    \n",
    "    max_num += 1\n",
    "    \n",
    "    test_size = round(test_size, 2)\n",
    "\n",
    "    test_size = 0.2\n",
    "#     num_iters = 600\n",
    "    SEED = 86 # - этот лучше всех для CatBoost, для других = 17\n",
    "\n",
    "    print(f'test_size: {test_size} SEED={SEED}')\n",
    "\n",
    "    # Split the train_df into training and testing sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train, target,\n",
    "                                                          test_size=test_size,\n",
    "                                                          stratify=target,\n",
    "                                                          random_state=SEED)\n",
    "\n",
    "    # # Split the train_df into training and testing sets\n",
    "    # tscv = TimeSeriesSplit(test_size=0.25)\n",
    "    # train_idxs, valid_idxs = tscv.split(train)\n",
    "    # print(train_idxs)\n",
    "\n",
    "    # # самодельный train_test_split\n",
    "    # X_train, X_valid, y_train, y_valid = data_cls.train_test_split(train, target)\n",
    "\n",
    "    pool_train = Pool(data=X_train, label=y_train, cat_features=cat_columns)\n",
    "    pool_valid = Pool(data=X_valid, label=y_valid, cat_features=cat_columns)\n",
    "    pool_test = Pool(data=test_df, cat_features=cat_columns)\n",
    "\n",
    "    num_folds = 4\n",
    "    skf = StratifiedKFold(n_splits=num_folds)\n",
    "    split_kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    fit_on_full_train = True\n",
    "    use_grid_search = False\n",
    "    use_cv_folds = False\n",
    "    build_model = True\n",
    "    stratified = True\n",
    "\n",
    "    models, models_scores, predict_scores = [], [], []\n",
    "\n",
    "    loss_function = 'MultiClass'\n",
    "    # loss_function = 'MultiClassOneVsAll'\n",
    "\n",
    "    # auto_class_weights = 'Balanced'\n",
    "    auto_class_weights = None\n",
    "\n",
    "    eval_metric = 'TotalF1'\n",
    "#     eval_metric = 'Accuracy'\n",
    "#     eval_metric = 'AUC:hints=skip_train~false'    \n",
    "\n",
    "    clf_params = dict(cat_features=cat_columns,\n",
    "                      auto_class_weights=auto_class_weights,\n",
    "                      loss_function=loss_function,\n",
    "                      eval_metric=eval_metric,\n",
    "                      # iterations=2000,  # попробовать столько итераций\n",
    "                      early_stopping_rounds=80,\n",
    "                      random_seed=SEED,\n",
    "                      task_type=\"GPU\",\n",
    "                      devices='0:1',\n",
    "                      )\n",
    "\n",
    "    clf = CatBoostClassifier(**clf_params)\n",
    "\n",
    "    if use_grid_search:\n",
    "        # grid_params = {\n",
    "        #     'max_depth': [5, 6],\n",
    "        #     'learning_rate': [0.1, 0.15, 0.2],\n",
    "        # }\n",
    "        # grid_search_result = clf.grid_search(grid_params, train, target,\n",
    "        #                                      cv=skf,\n",
    "        #                                      stratified=True,\n",
    "        #                                      refit=True,\n",
    "        #                                      plot=True,\n",
    "        #                                      verbose=100,\n",
    "        #                                      )\n",
    "        # best_params = grid_search_result['params']\n",
    "        # models.append(clf)\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
    "        )\n",
    "        study.optimize(objective, n_trials=12, timeout=600)\n",
    "\n",
    "        print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "        print(\"Best trial:\")\n",
    "        trial = study.best_trial\n",
    "        print(\"  Value: {}\".format(trial.value))\n",
    "        print(\"  Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "        best_params = trial.params\n",
    "\n",
    "        clf_params.update(best_params)\n",
    "        print('clf_params', clf_params)\n",
    "\n",
    "        clf = CatBoostClassifier(**clf_params)\n",
    "\n",
    "    if use_cv_folds:\n",
    "        if stratified:\n",
    "            skf_folds = skf.split(train, target)\n",
    "        else:\n",
    "            skf_folds = split_kf.split(train)\n",
    "\n",
    "        for idx, (train_idx, valid_idx) in enumerate(skf_folds, 1):\n",
    "            print(f'Фолд {idx} из {num_folds}')\n",
    "            train_data = Pool(data=train.iloc[train_idx],\n",
    "                              label=target.iloc[train_idx],\n",
    "                              cat_features=cat_columns)\n",
    "            valid_data = Pool(data=train.iloc[valid_idx],\n",
    "                              label=target.iloc[valid_idx],\n",
    "                              cat_features=cat_columns)\n",
    "            model = clf\n",
    "            model.fit(train_data, eval_set=valid_data, use_best_model=True, verbose=100)\n",
    "            models.append(model)\n",
    "            if build_model:\n",
    "                DTS = (X_train, X_valid, y_train, y_valid, train, target, test_df)\n",
    "                predict_test(idx, model, DTS, max_num, submit_prefix='cb_', \n",
    "                             save_predict_proba=save_predict_proba)\n",
    "\n",
    "        best_params = {'iterations': [clf.tree_count_ for clf in models]}\n",
    "\n",
    "    else:\n",
    "        DTS = (X_train, X_valid, y_train, y_valid, train, target, test_df)\n",
    "\n",
    "        clf.fit(pool_train, eval_set=pool_valid, use_best_model=True, verbose=50)\n",
    "\n",
    "        models.append(clf)\n",
    "\n",
    "        best_params = {'clf_iters': clf.tree_count_,\n",
    "                       'clf_lr': clf.get_all_params()['learning_rate']}\n",
    "\n",
    "        if build_model:\n",
    "            if not fit_on_full_train:\n",
    "                predict_scores = predict_test(0, clf, DTS, max_num, submit_prefix='CB_',\n",
    "                                             save_predict_proba=save_predict_proba)\n",
    "\n",
    "            else:\n",
    "                predict_scores = predict_test('pool', clf, DTS, max_num, submit_prefix='CB_',\n",
    "                                             save_predict_proba=save_predict_proba)\n",
    "                acc_train, acc_valid, acc_full, roc_auc, f1w = predict_scores\n",
    "                score = acc_valid\n",
    "                comment = {'times': prefix_preprocess,\n",
    "                           'test_size': test_size, \n",
    "                           'SEED': SEED,\n",
    "                           'size': 'pool',\n",
    "                           'clf_iters': models[0].best_iteration_,\n",
    "                           'clf_lr': models[0].get_all_params()['learning_rate'],\n",
    "                           'stratified': stratified}                \n",
    "                comment.update(data_cls.comment)\n",
    "                comment.update(models[0].get_params())\n",
    "\n",
    "                with open(file_logs, mode='a') as log:\n",
    "                    # log.write('num;mdl;roc_auc;acc_train;acc_valid;acc_full;score;WF1;'\n",
    "                    #           'model_columns;exclude_columns;cat_columns;comment\\n')\n",
    "                    log.write(f'{max_num};CB;{roc_auc:.6f};{acc_train:.6f};{acc_valid:.6f};'\n",
    "                              f'{acc_full:.6f};'\n",
    "                              f'{score:.6f};{f1w:.6f};{train_df.columns.tolist()};'\n",
    "                              f'{data_cls.exclude_columns};{cat_columns};{comment}\\n')\n",
    "\n",
    "                # обучение на всем трейне\n",
    "                print('Обучаюсь на всём трейне...')\n",
    "                clf_params['iterations'] = int(clf.tree_count_ * 1.1)\n",
    "                clf_params['learning_rate'] = clf.get_all_params()['learning_rate']\n",
    "                model = CatBoostClassifier(**clf_params)\n",
    "                model.fit(train, target, verbose=50, cat_features=cat_columns)\n",
    "                predict_scores = predict_test('full', model, DTS, max_num,\n",
    "                                              submit_prefix='cb_', \n",
    "                                              save_predict_proba=save_predict_proba)\n",
    "\n",
    "        best_params.update(clf.get_params())\n",
    "\n",
    "    print('best_params:', best_params)\n",
    "\n",
    "    if build_model:\n",
    "        if len(models) > 1:\n",
    "            predict_scores = [np.mean(arg) for arg in zip(*models_scores)]\n",
    "\n",
    "        acc_train, acc_valid, acc_full, roc_auc, f1w = predict_scores\n",
    "        score = acc_valid\n",
    "\n",
    "        print(f'Weighted F1-score = {f1w:.6f}')\n",
    "        print('Параметры модели:', clf.get_params())\n",
    "\n",
    "        print_time(start_time)\n",
    "\n",
    "        comment = {'times': prefix_preprocess,\n",
    "                   'test_size': test_size,\n",
    "                   'SEED': SEED,\n",
    "                   'clf_iters': models[0].best_iteration_,\n",
    "                   'clf_lr': models[0].get_all_params()['learning_rate'],\n",
    "                   'stratified': stratified}\n",
    "        comment.update(data_cls.comment)\n",
    "        comment.update(models[0].get_params())\n",
    "\n",
    "        with open(file_logs, mode='a') as log:\n",
    "            # log.write('num;mdl;roc_auc;acc_train;acc_valid;acc_full;score;WF1;'\n",
    "            #           'model_columns;exclude_columns;cat_columns;comment\\n')\n",
    "            log.write(f'{max_num};CB;{roc_auc:.6f};{acc_train:.6f};{acc_valid:.6f};'\n",
    "                      f'{acc_full:.6f};'\n",
    "                      f'{score:.6f};{f1w:.6f};{train_df.columns.tolist()};'\n",
    "                      f'{data_cls.exclude_columns};{cat_columns};{comment}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-01T12:03:13.881729Z",
     "iopub.execute_input": "2023-05-01T12:03:13.882058Z",
     "iopub.status.idle": "2023-05-01T12:03:34.361860Z",
     "shell.execute_reply.started": "2023-05-01T12:03:13.882029Z",
     "shell.execute_reply": "2023-05-01T12:03:34.360797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cb_proba_449_full.csv\n./CB_submit_449_pool.csv\n./CB_train_449_pool.csv\n./cb_train_449_full.csv\n./CB_proba_449_pool.csv\n./cb_submit_449_full.csv\n./scores.logs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='predictions.zip' target='_blank'>predictions.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/predictions.zip"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, FileLink\n",
    "from zipfile import ZipFile, ZIP_DEFLATED as ZD\n",
    "from glob import glob\n",
    "\n",
    "files = glob(f'{PREDICTIONS_DIR}/*.csv') + glob(f'{PREDICTIONS_DIR}/*.logs')\n",
    "zip_filename = WORK_PATH.joinpath('predictions.zip')\n",
    "with ZipFile(zip_filename, 'w',  compression=ZD, compresslevel=9) as zip_file:\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        zip_file.write(filename)\n",
    "FileLink(zip_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
